# Benchmarks Directory Structure

This directory contains the Pyomo benchmark infrastructure for LyoPRONTO.

## ğŸ“ Directory Organization

```
benchmarks/
â”œâ”€â”€ src/                    # Reusable Python modules (import as benchmarks.src)
â”‚   â”œâ”€â”€ __init__.py        # Package initialization
â”‚   â”œâ”€â”€ paths.py           # Centralized path resolution
â”‚   â”œâ”€â”€ data_loader.py     # JSONL parsing and organization
â”‚   â”œâ”€â”€ analyze_benchmark.py  # Metrics computation
â”‚   â”œâ”€â”€ visualization.py   # Plot generation
â”‚   â”œâ”€â”€ adapters.py        # Data source adapters
â”‚   â”œâ”€â”€ scenarios.py       # Benchmark scenario definitions
â”‚   â””â”€â”€ schema.py          # Data validation schemas
â”‚
â”œâ”€â”€ scripts/               # CLI entry points and orchestration
â”‚   â”œâ”€â”€ grid_cli.py        # Main CLI for benchmark generation
â”‚   â”œâ”€â”€ generate_reports.py  # Analysis report generator
â”‚   â”œâ”€â”€ run_grid.py        # Grid search runner
â”‚   â”œâ”€â”€ run_batch.py       # Batch execution
â”‚   â”œâ”€â”€ validate.py        # Result validation
â”‚   â””â”€â”€ repair_failed_runs.py  # Retry failed benchmarks
â”‚
â”œâ”€â”€ notebooks/             # Interactive analysis notebooks
â”‚   â”œâ”€â”€ grid_analysis_SIMPLE.ipynb  # Primary analysis viewer
â”‚   â””â”€â”€ grid_analysis.ipynb         # Advanced analysis (WIP)
â”‚
â”œâ”€â”€ results/               # Versioned benchmark results
â”‚   â””â”€â”€ <version>/         # e.g., "test", "v1_baseline", "v2_free_initial"
â”‚       â”œâ”€â”€ raw/           # Original benchmark outputs (JSONL)
â”‚       â”œâ”€â”€ processed/     # Derived data (CSV, JSON summaries)
â”‚       â””â”€â”€ figures/       # Generated plots (PNG)
â”‚           â”œâ”€â”€ Tsh/       # Shelf temperature optimization plots
â”‚           â”œâ”€â”€ Pch/       # Chamber pressure optimization plots
â”‚           â””â”€â”€ both/      # Joint Tsh+Pch optimization plots
â”‚
â”œâ”€â”€ archive/               # Historical artifacts
â”‚   â”œâ”€â”€ legacy_notebooks/  # Superseded notebooks
â”‚   â”œâ”€â”€ superseded_figures/  # Old/combined plot versions
â”‚   â””â”€â”€ README.md          # Archive documentation
â”‚
â”œâ”€â”€ tests/                 # Integration tests for benchmark infra
â”‚
â”œâ”€â”€ cleanup.py             # Automated maintenance utility
â””â”€â”€ README.md              # This file
```

## ğŸš€ Quick Start

### Running Benchmarks

```bash
# From benchmarks/ directory:

# 1. Generate a 2Ã—2 grid benchmark for Tsh optimization
python scripts/grid_cli.py generate \
    --task Tsh \
    --param product.A1 10 20 \
    --param ht.KC 2e-4 4e-4 \
    --version test

# 2. Generate analysis reports (heatmaps, trajectories, summaries)
python scripts/generate_reports.py results/test

# 3. View results in notebook
jupyter notebook notebooks/grid_analysis_SIMPLE.ipynb
```

### Viewing Existing Results

```bash
# Open the notebook and set:
benchmark_version = "test"  # or "v1_baseline", etc.
task = "Tsh"  # or "Pch", "both"

# Run all cells to display:
# - Objective difference heatmaps (% vs scipy)
# - Speedup heatmaps (wall clock time)
# - Trajectory comparisons
# - Summary statistics
```

## ğŸ“Š Result Structure

Each benchmark version follows this pattern:

```
results/<version>/
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ Tsh_2x2_test.jsonl          # Raw benchmark data
â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ summary.json                 # Aggregated statistics
â”‚   â””â”€â”€ comparison_table.csv         # Detailed comparison table
â””â”€â”€ figures/
    â””â”€â”€ Tsh/
        â”œâ”€â”€ objective_diff_heatmap_fd.png       # FD objective difference
        â”œâ”€â”€ objective_diff_heatmap_colloc.png   # Collocation objective difference
        â”œâ”€â”€ speedup_heatmap_fd.png              # FD speedup
        â”œâ”€â”€ speedup_heatmap_colloc.png          # Collocation speedup
        â”œâ”€â”€ nominal_trajectory_shelf_temperature.png      # Tsh trajectory
        â”œâ”€â”€ nominal_trajectory_dried_fraction.png         # Dried fraction trajectory
        â”œâ”€â”€ nominal_ramp_constraints.png        # Ramp rate validation
        â””â”€â”€ speedup_barplot.png                 # Summary bar chart
```

## ğŸ”§ Module Usage

### Path Resolution (Recommended)

```python
from benchmarks.src.paths import get_results_dir, get_figures_dir, get_processed_dir

# Get versioned directories
raw_dir = get_results_dir("v1_baseline") / "raw"
figures_dir = get_figures_dir("v1_baseline", "Tsh")
processed_dir = get_processed_dir("v1_baseline")
```

**Benefits:**
- No hardcoded paths
- Consistent across scripts and notebooks
- Automatic path creation with `ensure_dir()`

### Data Loading

```python
from benchmarks.src.data_loader import load_benchmark_jsonl, organize_by_method

# Load benchmark results
records = load_benchmark_jsonl("results/test/raw/Tsh_2x2.jsonl")
by_method = organize_by_method(records)  # {'scipy': [...], 'fd': [...], 'colloc': [...]}
```

### Analysis

```python
from benchmarks.src.analyze_benchmark import compute_objective_differences, compute_speedups

obj_diff = compute_objective_differences(by_method)
speedups = compute_speedups(by_method)
```

### Visualization

```python
from benchmarks.src.visualization import plot_objective_diff_heatmaps, plot_trajectory_comparison

# Returns dict of saved files: {'fd': Path, 'colloc': Path}
heatmap_files = plot_objective_diff_heatmaps(obj_diff, output_dir=figures_dir)

# Returns dict: {'Tsh': Path, 'Pch': Path, 'Dried Fraction': Path}
traj_files = plot_trajectory_comparison(nominal_case, output_dir=figures_dir)
```

## ğŸ§¹ Maintenance

### Cleanup Utility

```bash
# Check for duplicates and naming violations (read-only)
python cleanup.py --version test --check-only

# Archive superseded files
python cleanup.py --version test --archive-duplicates

# Generate artifact manifest (inventory of all files)
python cleanup.py --version test --generate-manifest
```

**What it does:**
- Detects combined heatmaps when split versions exist
- Validates file naming conventions
- Archives duplicates to `archive/superseded_figures/`
- Generates `manifest.json` (file inventory with hashes/sizes)

### Naming Conventions

**Figures:**
- Trajectories: `nominal_trajectory_<variable>.png` (e.g., `nominal_trajectory_shelf_temperature.png`)
- Heatmaps: `<metric>_heatmap_<method>.png` (e.g., `objective_diff_heatmap_fd.png`)
- Summaries: `speedup_barplot.png`, `nominal_ramp_constraints.png`

**Data:**
- Raw: `<task>_<grid_size>_<label>.jsonl` (e.g., `Tsh_3x3_baseline.jsonl`)
- Processed: `comparison_table.csv`, `summary.json`, `manifest.json`

### Git Ignore Policy

- **Tracked:** Baseline results (`results/baseline/`), manifests, key figures
- **Ignored:** Test runs (`results/test/`), temporary files (`.tmp`), logs
- **Committed:** Source code in `src/`, `scripts/`, `notebooks/`

## ğŸ—‚ï¸ Migration from Old Structure

The previous structure mixed source and generated artifacts:

```
# OLD (before cleanup)
benchmarks/
â”œâ”€â”€ data_loader.py         # âŒ Root-level modules
â”œâ”€â”€ visualization.py
â”œâ”€â”€ grid_cli.py            # âŒ Scripts mixed with modules
â”œâ”€â”€ both_grid_heatmaps.png # âŒ Generated files in root
analysis/                  # âŒ Duplicate structure
â””â”€â”€ test/Tsh/*.png

# NEW (after cleanup)
benchmarks/
â”œâ”€â”€ src/                   # âœ… Packaged modules
â”œâ”€â”€ scripts/               # âœ… Separated CLI tools
â”œâ”€â”€ results/test/figures/  # âœ… Unified artifact location
â””â”€â”€ archive/               # âœ… Historical artifacts preserved
```

**Key improvements:**
1. **Separation:** Source code (`src/`) vs executables (`scripts/`) vs artifacts (`results/`)
2. **Versioning:** Results organized by version, not scattered
3. **Scalability:** Adding Pyomo methods requires no code changes (data-driven)
4. **Maintainability:** Automated cleanup, manifest tracking, `.gitignore` patterns

## ğŸ“š Related Documentation

- `src/paths.py` - Path resolution API reference
- `archive/README.md` - Archive policy and contents
- `cleanup.py --help` - Maintenance utility usage
- `notebooks/grid_analysis_SIMPLE.ipynb` - Interactive analysis examples

## ğŸ”„ Workflow Summary

1. **Generate:** `scripts/grid_cli.py generate ...` â†’ `results/<version>/raw/*.jsonl`
2. **Analyze:** `scripts/generate_reports.py results/<version>` â†’ `processed/` + `figures/`
3. **View:** Open `notebooks/grid_analysis_SIMPLE.ipynb`, set version/task, run cells
4. **Maintain:** Run `cleanup.py` periodically to archive duplicates and validate naming
5. **Version:** Create new version dirs for major benchmark runs (e.g., `v2_new_solver`)

## ğŸ’¡ Best Practices

- **Use path helpers:** Import from `src.paths` instead of hardcoding
- **Version everything:** Create distinct `results/<version>/` for each major run
- **Archive old runs:** Compress with `cleanup.py` after 6 months
- **Check manifests:** Run `--generate-manifest` to inventory artifacts
- **Dynamic discovery:** Notebooks auto-detect Pyomo methods from `by_method` dict
- **Split over combined:** Prefer per-method/per-variable plots for scalability

## ğŸ› Troubleshooting

**Import errors after restructure:**
```python
# âŒ Old:
from data_loader import load_benchmark_jsonl

# âœ… New:
from benchmarks.src.data_loader import load_benchmark_jsonl
# OR (from within benchmarks/)
from src.data_loader import load_benchmark_jsonl
```

**Path not found:**
```python
# âŒ Old:
analysis_dir = Path("analysis/test/Tsh")

# âœ… New:
from src.paths import get_figures_dir
figures_dir = get_figures_dir("test", "Tsh")
```

**Missing plots in notebook:**
- Check `figures_dir` path matches task
- Re-run `generate_reports.py` to regenerate
- Verify JSONL exists in `results/<version>/raw/`

## ğŸ¯ Future Enhancements

- [ ] Automated compression of old result versions (tar.gz)
- [ ] Pre-commit hook for naming validation
- [ ] Parameter set IDs in filenames (for multi-grid runs)
- [ ] Interactive plotly versions of heatmaps
- [ ] CI integration for benchmark regression testing
